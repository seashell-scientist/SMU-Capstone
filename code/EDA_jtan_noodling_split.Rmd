---
title: "Capstone"
author: "Jenna Ford, Christian Nava, Jonathan Tan"
date: "5/21/2020"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
library(rmdformats)
library(tidyverse)  # data manipulation
library(data.table)
library(tswge)  # Time series package
library(tseries)  # for Dickey-Fuller test 
library(orcutt)  # for Cochrane-Orcutt test
library(formattable)  # for table formatting
library(GGally)
library(astsa)
library(nnfor)
library(dplyr)
library(ggplot2)
library(changepoint)
library(date)
knitr::opts_chunk$set(echo = TRUE,
               prompt = FALSE,
               tidy = TRUE,
               comment = NA,
               message = FALSE,
               warning = FALSE)
```

***
# Time Series Analysis of Corpus Christi Liquor Distribution
***
Jenna Ford, Christian Nava and Jonathan Tan  
May 21, 2020


## Read in Datasets and Combine
***
```{r data} 
#note ask for updated headings csv changes to stay synced
df_17_19 = read.csv("D:/SMU/DS 6120 Capstone A/Bivin CC 2017-19_updated_headings.csv")
df_15_16 = read.csv("D:/SMU/DS 6120 Capstone A/Bivin CC 2015-16_updated_headings.csv")
df_13_14 = read.csv("D:/SMU/DS 6120 Capstone A/Bivin CC 2013-14_updated_headings.csv")
#df_17_19 = read.csv("C:/Users/b007224/Documents/masters_in_data_science/capstone/data/Bivin CC 2017-19_updated_headings.csv")
#df_15_16 = read.csv("C:/Users/b007224/Documents/masters_in_data_science/capstone/data/Bivin CC 2015-16_updated_headings.csv")
#df_13_14 = read.csv("C:/Users/b007224/Documents/masters_in_data_science/capstone/data/Bivin CC 2013-14_updated_headings.csv")

dim(df_17_19)
dim(df_15_16)
dim(df_13_14)

#format date column
df_17_19$date = date.mmddyy(mdy.date(match(substr(df_17_19$Month, 1, 3),month.abb),01,df_17_19$Year),sep="/")
df_15_16$date = date.mmddyy(mdy.date(match(substr(df_15_16$Month, 4, 6),month.abb),01,df_15_16$Year),sep="/")
df_13_14$date = date.mmddyy(mdy.date(match(substr(df_13_14$Month, 1, 3),month.abb),01,df_13_14$Year),sep="/")

df <- rbind(df_17_19,df_15_16,df_13_14)

dim(df)
```

## Check for Missing Data
***
Here we create a function to check for missing data.
```{r missing}
check.for.missing.data <- function(data){
  a = colnames(data)
  b = colSums(is.na(df))  %>% as.data.table
  missing_value_table = cbind(a, b)
  colnames(missing_value_table) = c("Variables","Missing_values")
  missing_value_table = missing_value_table  %>% filter(Missing_values>0)  %>% 
                        mutate("As a % of Total Values" = round(100 * (Missing_values / nrow(df)), 1))  %>% 
                        arrange(desc(Missing_values))
  head(missing_value_table, 20)
}

table_a = check.for.missing.data(data=df)
# display table with first column aligned left all others aligned right
formattable(table_a, align = c("l", rep("r", NCOL(table_a) - 1)))
```

## Keep Only Active Accounts
***

```{r filter}
invisible(df %>% filter(Account_Status != "Closed"))
```

## Drop Variables
***

```{r drop}
drops <- c("Metrics","Year","Month","House","Account_Status","Beverage_Type","Fiscal_Year","Premise","Customer_Street_Address","Customer_City",
           "Customer_Zip_Code","Longitude_Customer","Latitude_Customer","Customer","Vendor","Brand_ID","Brand","Size","Product_ID","Chain","Category",
           "Product_Type_ID","Qty_Per_Case","Alcohol_Proof","X9L_Cases","Dollar_Sales_Per_Case","Dollar_Sales_Per_9L_Case")
df = df[ , !(names(df) %in% drops)]
```

## Variable Formatting
***

```{r formatting}
df$Dollar_Sales = as.numeric(gsub('[$,]', '', df$Dollar_Sales))
str(df)
```

## Products
***

```{r products}
product_type_group = df %>% group_by(Product_Type) %>% tally(sort=TRUE)
dim(product_type_group)
formattable(head(product_type_group), align = c("l", rep("r", NCOL(table_a) - 1)))

product_group = df %>% group_by(Product) %>% tally(sort=TRUE)
dim(product_group)
formattable(head(product_group), align = c("l", rep("r", NCOL(table_a) - 1)))

customer_group = df %>% group_by(Customer_ID) %>% tally(sort=TRUE)
dim(customer_group)
formattable(head(customer_group), align = c("l", rep("r", NCOL(table_a) - 1)))

product_type_product_group = df %>% group_by(Product_Type, Product) %>% tally(sort=TRUE)
dim(product_type_product_group)
formattable(head(product_type_product_group), align = c("l", rep("r", NCOL(table_a) - 1)))

all_group = df %>% group_by(Product_Type, Product, Customer_ID) %>% tally(sort=TRUE)
dim(all_group)
formattable(head(all_group), align = c("l", rep("r", NCOL(table_a) - 1)))
```

## Put Combinations for Forecasting into a Dataframe and Choose 10 Sample Combinations
***

```{r combinations}
# Create file with all possible combinations
combinations0 = as.data.frame(df %>% group_by(Product_Type, Product, Customer_ID) %>% tally(sort=TRUE))
combinations = combinations0 %>% filter(n >= 42)

# Sample combinations
set.seed(123)
sample_combinations = sample_n(combinations,10)
drops <- c("n")
sample_combinations = sample_combinations[ , !(names(sample_combinations) %in% drops)]
formattable(sample_combinations, align = c("l", rep("r", NCOL(table_a) - 1)))
```

## Time Series EDA
***

```{r eda1}
# Find data from file for first combination
all_dates = data.frame(date=c("1/1/2013","2/1/2013","3/1/2013","4/1/2013","5/1/2013","6/1/2013","7/1/2013","8/1/2013","9/1/2013","10/1/2013","11/1/2013","12/1/2013",
                              "1/1/2014","2/1/2014","3/1/2014","4/1/2014","5/1/2014","6/1/2014","7/1/2014","8/1/2014","9/1/2014","10/1/2014","11/1/2014","12/1/2014",
                              "1/1/2015","2/1/2015","3/1/2015","4/1/2015","5/1/2015","6/1/2015","7/1/2015","8/1/2015","9/1/2015","10/1/2015","11/1/2015","12/1/2015",
                              "1/1/2016","2/1/2016","3/1/2016","4/1/2016","5/1/2016","6/1/2016","7/1/2016","8/1/2016","9/1/2016","10/1/2016","11/1/2016","12/1/2016",
                              "1/1/2017","2/1/2017","3/1/2017","4/1/2017","5/1/2017","6/1/2017","7/1/2017","8/1/2017","9/1/2017","10/1/2017","11/1/2017","12/1/2017",
                              "1/1/2018","2/1/2018","3/1/2018","4/1/2018","5/1/2018","6/1/2018","7/1/2018","8/1/2018","9/1/2018","10/1/2018","11/1/2018","12/1/2018",
                              "1/1/2019","2/1/2019","3/1/2019","4/1/2019","5/1/2019","6/1/2019","7/1/2019","8/1/2019","9/1/2019","10/1/2019","11/1/2019","12/1/2019"))

date_combinations = merge(all_dates,sample_combinations,all=TRUE)
str(date_combinations)

# Join date/sample combinations with primary dataset to review records
temp = left_join(date_combinations,df)

# Replace missing values for STD_Cases and Dollar_Sales with zeros since no cases were sold those months
temp[is.na(temp)] <- 0
str(temp)

```

```{r sample1, eval=FALSE}
#only one sample combination
sample_combinations1 = sample_combinations[1,]
temp1 = inner_join(temp,sample_combinations1)
product = sample_combinations1$Product
customer = sample_combinations1$Customer_ID

plot.ts(temp1$STD_Cases, 
        main=c(paste("Standard Case Sales of ", product), 
               paste("for ",customer)),
        xlab="Days",
        ylab="Standard Cases")

par(mfrow = c(2,2))
invisible(acf(temp1$STD_Cases, main="ACF"))
invisible(parzen.wge(temp1$STD_Cases))

invisible(acf(temp1$STD_Cases[0:length(temp1$date)/2], main="ACF for 1st Half of Series"))
invisible(acf(temp1$STD_Cases[(1+length(temp1$date)/2):length(temp1$date)], main="ACF for 2nd Half of Series"))

aic = aic5.wge(temp1$STD_Cases,type="aic")
aic

#for (row in 1:nrow(aic)) {
for (row in 1:1) {
  if(aic$`   p` == 0 & aic$`   q` == 0){
    print("One of the top 5 models using BIC was an ARMA(0,0), indicating this series may be white noise.")
  }
}
```

```{r loop}
# loop through sample combinations
for(i in 1:10) {
  sample_combinations1 = sample_combinations[i,]
  temp1 = inner_join(temp,sample_combinations1)
  product = sample_combinations1$Product
  customer = sample_combinations1$Customer_ID
  
  par(mfrow=c(1,1))
  plot.ts(temp1$STD_Cases, 
          main=c(paste("Standard Case Sales of ", product), 
                 paste("for ",customer)),
          xlab="Days",
          ylab="Standard Cases")
  
  par(mfrow = c(2,2))
  invisible(acf(temp1$STD_Cases, main="ACF"))
  invisible(parzen.wge(temp1$STD_Cases))

  invisible(acf(temp1$STD_Cases[0:length(temp1$date)/2], main="ACF for 1st Half of Series"))
  invisible(acf(temp1$STD_Cases[(1+length(temp1$date)/2):length(temp1$date)], main="ACF for 2nd Half of Series"))

  sink("file")
  ljung_24 = ljung.wge(temp1$STD_Cases,K=24)
  sink()
  cat("The Ljung-Box test with K=24 has a chi-square value of",ljung_24$chi.square,".")
  
  sink("file")
  ljung_48 = ljung.wge(temp1$STD_Cases,K=48)
  sink()
  cat("The Ljung-Box test with K=48 has a chi-square value of",ljung_48$chi.square,".")
  
  if (ljung_24$chi.square < .05 & ljung_48$chi.square < .05){
    print("Ljung-Box test results: At a significance level of 0.05, we reject the null hypothesis that this dataset is white noise.")
  } else if (ljung_24$chi.square > .05 & ljung_48$chi.square < .05){
    print("Ljung-Box test results: At a significance level of 0.05, the test is inconclusive.")
  } else if (ljung_24$chi.square < .05 & ljung_48$chi.square > .05){
    print("Ljung-Box test results: At a significance level of 0.05, the test is inconclusive.")
  } else {
    print("Ljung-Box test results: At a significance level of 0.05, we fail to reject the null hypothesis that this dataset is white noise.")
  }
  
  aic = invisible(aic5.wge(temp1$STD_Cases,type="bic"))
  
  #for (row in 1:nrow(aic)) {
  for (row in 1:1) {
    if(aic$`   p` == 0 & aic$`   q` == 0){
      print("One of the top 5 models using BIC was an ARMA(0,0), indicating this series may be white noise.")
    }
}
}
```

```{r}
# Core Tidyverse
library(tidyverse)
library(glue)
library(forcats)

# Time Series
library(timetk)
library(tidyquant)
library(tibbletime)

# Visualization
library(cowplot)

# Preprocessing
library(recipes)

# Sampling / Accuracy
library(rsample)
library(yardstick) 

# Modeling
library(keras)
library(tfruns)
```

format sample combination ts into tripartate LSTM format
first need ts form, set index and value
cross validation for time series: need to keep the time dependency, can't just shuffle things without losing meaning. create multiple synthetic "futures"? 

```{r}
ex1 <- data.frame(date = strptime((temp1$date), "%d/%m/%Y"), cases = temp1$STD_Cases)
ggplot(ex1, aes(x = date, y = cases)) + geom_line()
```

```{r}
ex1 <- data.frame(date = strptime((temp1$date), "%d/%m/%Y"), cases = temp1$STD_Cases)
ex2 <- tbl_time(ex1, date)

rolling_origin_resamples <- rolling_origin(
  ex2,
  initial    = 24,
  assess     = 12,
  cumulative = FALSE, #means that the resmaples will be the same... size? 
  skip       = 6
)

rolling_origin_resamples

```

backsample vizualization 
```{r}
# Plotting function for a single split
plot_split <- function(split, expand_y_axis = TRUE, 
                       alpha = 1, size = 1, base_size = 14) {
    
    # Manipulate data
    train_tbl <- training(split) %>%
        add_column(key = "training") 
    
    test_tbl  <- testing(split) %>%
        add_column(key = "testing") 
    
    data_manipulated <- bind_rows(train_tbl, test_tbl) %>%
        as_tbl_time(index = date) %>%
        mutate(key = fct_relevel(key, "training", "testing"))
        
    # Collect attributes
    train_time_summary <- train_tbl %>%
        tk_index() %>%
        tk_get_timeseries_summary()
    
    test_time_summary <- test_tbl %>%
        tk_index() %>%
        tk_get_timeseries_summary()
    
    # Visualize
    g <- data_manipulated %>%
        ggplot(aes(x = date, y = cases, color = key)) +
        geom_line(size = size, alpha = alpha) +
        theme_tq(base_size = base_size) +
        scale_color_tq() +
        labs(
          title    = glue("Split: {split$id}"),
          subtitle = glue("{train_time_summary$start} to ", 
                          "{test_time_summary$end}"),
            y = "", x = ""
        ) +
        theme(legend.position = "none") 
    
    if (expand_y_axis) {
        
        time_summary <- ex2 %>% 
            tk_index() %>% 
            tk_get_timeseries_summary()
        
        g <- g +
            scale_x_date(limits = c(time_summary$start, 
                                    time_summary$end))
    }
    
    g
}
```
shows a single split that has 100 years train, 50 test. if you plotted the second split, this entire "window" would move up by 22 years
```{r}
rolling_origin_resamples$splits[[1]] %>%
    plot_split(expand_y_axis = FALSE) +
    theme(legend.position = "bottom")
```

```{r}
#diagnosis cell
split <- rolling_origin_resamples$splits[[1]] 
alpha = 1
size = 1
base_size = 14


# Manipulate data
    train_tbl <- training(split) %>%
        add_column(key = "training") 
    
    test_tbl  <- testing(split) %>%
        add_column(key = "testing") 
    
    data_manipulated <- bind_rows(train_tbl, test_tbl) %>%
        as_tbl_time(index = date) %>%
        mutate(key = fct_relevel(key, "training", "testing"))
    
     # Collect attributes
    train_time_summary <- train_tbl %>%
        tk_index() %>%
        tk_get_timeseries_summary()
    
    test_time_summary <- test_tbl %>%
        tk_index() %>%
        tk_get_timeseries_summary()
    
    # Visualize
    g <- data_manipulated %>%
        ggplot(aes(x = date, y = cases, color = key)) +
        geom_line(size = size, alpha = alpha) +
        theme_tq(base_size = base_size) +
        scale_color_tq() +
        labs(
          title    = glue("Split: {split$id}"),
          subtitle = glue("{train_time_summary$start} to ", 
                          "{test_time_summary$end}"),
            y = "", x = ""
        ) +
        theme(legend.position = "none") 
    g
```

plotting all subdivisions
```{r}
# Plotting function that scales to all splits 
plot_sampling_plan <- function(sampling_tbl, expand_y_axis = FALSE, 
                               ncol = 3, alpha = 1, size = 1, base_size = 14, 
                               title = "Sampling Plan") {
    
    # Map plot_split() to sampling_tbl
    sampling_tbl_with_plots <- sampling_tbl %>%
        mutate(gg_plots = map(splits, plot_split, 
                              expand_y_axis = expand_y_axis,
                              alpha = alpha, base_size = base_size))
    
    # Make plots with cowplot
    plot_list <- sampling_tbl_with_plots$gg_plots 
    
    p_temp <- plot_list[[1]] + theme(legend.position = "bottom")
    legend <- get_legend(p_temp)
    
    p_body  <- plot_grid(plotlist = plot_list, ncol = ncol)
    
    p_title <- ggdraw() + 
        draw_label(title, size = 14, fontface = "bold", 
                   colour = palette_light()[[1]])
    
    g <- plot_grid(p_title, p_body, legend, ncol = 1, 
                   rel_heights = c(0.05, 1, 0.05))
    
    g
    
}

rolling_origin_resamples %>%
    plot_sampling_plan(expand_y_axis = F, ncol = 3, alpha = 1, size = 1, base_size = 10, 
                       title = "Backtesting Strategy: Rolling Origin Sampling Plan")
```
so we can see the 'slices' of the data are pretty unhelpful when looking at a mostly sparse time series

looking at an individual slice to test lstm
```{r}
example_split    <- rolling_origin_resamples$splits[[6]]
example_split_id <- rolling_origin_resamples$id[[6]]

plot_split(example_split, expand_y_axis = FALSE, size = 0.5) + 
  theme(legend.position = 'bottom') + ggtitle(glue("Split: {example_split_id}"))
```

data setup
```{r}
split_length <- length(analysis(example_split)$date) #number of total datapoints in the split
split_point <- round(split_length*.6) #take the first 60% of the sample as train

df_trn <- analysis(example_split)[1:split_point, , drop = FALSE] 
df_val <- analysis(example_split)[(split_point + 1):split_length, , drop = FALSE] #take the rest data as validation/test
df_tst <- assessment(example_split) #but this test data is... ??? 

df <- bind_rows(
  df_trn %>% add_column(key = "training"),
  df_val %>% add_column(key = "validation"),
  df_tst %>% add_column(key = "testing")
) %>%
  as_tbl_time(index = date)

df
```

LSTM works best with centered and scaled input data, use recipies package step_center, step_scale, and step_sqrt to center data on ???, scale it, and remove outliers
```{r}
rec_obj <- recipe(cases ~ ., df) %>%
    step_sqrt(cases) %>%
    step_center(cases) %>%
    step_scale(cases) %>%
    prep()

df_processed_tbl <- bake(rec_obj, df)

df_processed_tbl

```
record center "adjustments" so you can reverse the values for interpretation later
```{r}
center_history <- rec_obj$steps[[2]]$means["cases"]
scale_history <- rec_obj$steps[[3]]$sds["cases"]

c("center", center_history, "scale", scale_history)
```

Keras LSTM expects 3d array with num_samples, num_timesteps, and num_features
num_samples is the number of observations in set, goes in the arg "batch_size"
num_timesteps is the length of hidden state??? 
 - this hidden state is dependent on how far ahead you want to forecast, usually just literally input the number of steps to "step ahead" 
num-features is number of predictors. for this univariate example, = 1

```{r}
n_timesteps <- 3
n_predictions <- n_timesteps
batch_size <- 6

# functions used
build_matrix <- function(tseries, overall_timesteps) {
  t(sapply(1:(length(tseries) - overall_timesteps + 1), function(x) 
    tseries[x:(x + overall_timesteps - 1)]))
}

reshape_X_3d <- function(X) {
  dim(X) <- c(dim(X)[1], dim(X)[2], 1)
  X
}

# extract values from data frame
train_vals <- df_processed_tbl %>%
  filter(key == "training") %>%
  select(cases) %>%
  pull()
valid_vals <- df_processed_tbl %>%
  filter(key == "validation") %>%
  select(cases) %>%
  pull()
test_vals <- df_processed_tbl %>%
  filter(key == "testing") %>%
  select(cases) %>%
  pull()


# build the windowed matrices
train_matrix <-
  build_matrix(train_vals, n_timesteps + n_predictions)
valid_matrix <-
  build_matrix(valid_vals, n_timesteps + n_predictions)
test_matrix <- build_matrix(test_vals, n_timesteps + n_predictions)

# separate matrices into training and testing parts
# also, discard last batch if there are fewer than batch_size samples
# (a purely technical requirement)
X_train <- train_matrix[, 1:n_timesteps]
y_train <- train_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_train <- X_train[1:(nrow(X_train) %/% batch_size * batch_size), ]
y_train <- y_train[1:(nrow(y_train) %/% batch_size * batch_size), ]

X_valid <- valid_matrix[, 1:n_timesteps]
y_valid <- valid_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_valid <- X_valid[1:(nrow(X_valid) %/% batch_size * batch_size), ]
y_valid <- y_valid[1:(nrow(y_valid) %/% batch_size * batch_size), ]

X_test <- test_matrix[, 1:n_timesteps]
y_test <- test_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_test <- X_test[1:(nrow(X_test) %/% batch_size * batch_size), ]
y_test <- y_test[1:(nrow(y_test) %/% batch_size * batch_size), ]
# add on the required third axis
X_train <- reshape_X_3d(matrix(X_train)) #note, x_train is coming out as a list rather than a matrix??? should be a 3d matrix
X_valid <- reshape_X_3d(matrix(X_valid)) #does matrixing the list work?? 
X_test <- reshape_X_3d(matrix(X_test))

y_train <- reshape_X_3d(matrix(y_train))
y_valid <- reshape_X_3d(matrix(y_valid))
y_test <- reshape_X_3d(matrix(y_test))
```

building the actual LSTM model
```{r}
FLAGS <- flags(
  flag_boolean('stateful', FALSE), #??? 
  flag_boolean('stack_layers', FALSE), #adding layers does ??? guides says it doesn't help for this task
  flag_integer('batch_size', 10), #samples fed to model per iteration
  flag_integer('n_timesteps', 12), #size of predictions, which is the same as size of hidden state
  flag_integer('n_epochs', 100), #how many epochs??? (time intervals?)
  flag_numeric('dropout', 0.2), #drops .2 units for linear transformation of inputs
  flag_numeric('recurrent_dropout', 0.2), #drops .2 units for linear transformation??? 
  flag_string('loss', 'logcosh'), #loss function used, logcosh worked better than MSE
  flag_string('optimizer_type', 'sgd'), #use stochastic gradient descent for optimization (other options include adam or rmsprop)
  flag_integer('n_units', 128), #size of lstm layer
  flag_numeric('lr', 0.003), #learning rate?
  flag_numeric('momentum', 0.9), #parameter for SGD optimizer
  flag_integer('patience', 10) #early stopping callback check
)

# the number of predictions we'll make equals the length of the hidden state
n_predictions <- FLAGS$n_timesteps
# how many features = predictors we have
n_features <- 1

optimizer <- switch(FLAGS$optimizer_type,
                    sgd = optimizer_sgd(lr = FLAGS$lr, 
                    momentum = FLAGS$momentum))

#early stop function if the loss starts "stalling"? 
callbacks <- list(
  callback_early_stopping(patience = FLAGS$patience)
)

```
note, requires CUDA installation to be in/on the system PATH

actually create model
```{r}
model <- keras_model_sequential()

model %>%
  layer_lstm(
    units = FLAGS$n_units, 
    # the first layer in a model needs to know the shape of the input data
    batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),
    dropout = FLAGS$dropout,
    recurrent_dropout = FLAGS$recurrent_dropout,
    # by default, an LSTM just returns the final state
    return_sequences = TRUE
  ) %>% time_distributed(layer_dense(units = 1))

model %>%
  compile(
    loss = FLAGS$loss,
    optimizer = optimizer,
    # in addition to the loss, Keras will inform us about current 
    # MSE while training
    metrics = list("mean_squared_error")
  )

history <- model %>% fit(
  x          = X_train,
  y          = y_train,
  validation_data = list(X_valid, y_valid),
  batch_size = FLAGS$batch_size,
  epochs     = FLAGS$n_epochs,
  callbacks = callbacks
)
```
convergence(?) varies strongly based on batch size, steps, skips, ect

```{r}
history$metrics
```


look at model vs training set
```{r}
pred_train <- model %>%
  predict(X_train, batch_size = FLAGS$batch_size) %>%
  .[, , 1]

# Retransform values to original scale
pred_train <- (pred_train * scale_history + center_history) ^2
compare_train <- df %>% filter(key == "training")
```

how to output what the actual forecasts are??? RMSE/metric evaluation?? 

```{r}
# build a dataframe that has both actual and predicted values
for (i in 1:nrow(pred_train)) {
  varname <- paste0("pred_train", i)
  compare_train <-
    mutate(compare_train,!!varname := c(
      rep(NA, FLAGS$n_timesteps + i - 1),
      pred_train[i,],
      rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)
    ))
}
```

```{r}
#future forecasts using temp1$STD_Cases


predict_keras_lstm_future <- function(data, epochs = 300, ...) {
    
    lstm_prediction <- function(data, epochs, ...) {
        
        # 5.1.2 Data Setup (MODIFIED)
        df <- temp1
        
        # 5.1.3 Preprocessing
        rec_obj <- recipe(STD_Cases ~ ., df) %>%
            step_sqrt(STD_Cases) %>%
            step_center(STD_Cases) %>%
            step_scale(STD_Cases) %>%
            prep()
        
        df_processed_tbl <- bake(rec_obj, df)
        
        center_history <- rec_obj$steps[[2]]$means["STD_Cases"]
        scale_history  <- rec_obj$steps[[3]]$sds["STD_Cases"]
        
        # 5.1.4 LSTM Plan
        lag_setting  <- 12 # = nrow(df_tst)
        batch_size   <- 36
        train_length <- 84
        tsteps       <- 1
        epochs       <- epochs
        
        # 5.1.5 Train Setup (MODIFIED)
        lag_train_tbl <- df_processed_tbl %>%
            mutate(STD_Cases_lag = lag(STD_Cases, n = lag_setting)) %>%
            filter(!is.na(STD_Cases_lag)) %>%
            tail(train_length)
        
        x_train_vec <- lag_train_tbl$STD_Cases_lag
        x_train_arr <- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))
        
        y_train_vec <- lag_train_tbl$STD_Cases
        y_train_arr <- array(data = y_train_vec, dim = c(length(y_train_vec), 1))
        
        x_test_vec <- y_train_vec %>% tail(lag_setting)
        x_test_arr <- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))
                
        # 5.1.6 LSTM Model
        model <- keras_model_sequential()

        model %>%
            layer_lstm(units            = 50, 
                       input_shape      = c(tsteps, 1), 
                       batch_size       = batch_size,
                       return_sequences = TRUE, 
                       stateful         = TRUE) %>% 
            layer_lstm(units            = 50, 
                       return_sequences = FALSE, 
                       stateful         = TRUE) %>% 
            layer_dense(units = 1)
        
        model %>% 
            compile(loss = 'mae', optimizer = 'adam')
        
        # 5.1.7 Fitting LSTM
        for (i in 1:epochs) {
            model %>% fit(x          = x_train_arr, 
                          y          = y_train_arr, 
                          batch_size = batch_size,
                          epochs     = 1, 
                          verbose    = 1, 
                          shuffle    = FALSE)
            
            model %>% reset_states()
            cat("Epoch: ", i)
            
        }
        
        # 5.1.8 Predict and Return Tidy Data (MODIFIED)
        # Make Predictions
        pred_out <- model %>% 
            predict(x_test_arr, batch_size = batch_size) %>%
            .[,1] #local py_call_error? arg mismatch here
        
        # Make future index using tk_make_future_timeseries()
        idx <- data %>%
            tk_index() %>%
            tk_make_future_timeseries(n_future = lag_setting)
        
        # Retransform values
        pred_tbl <- tibble(
            index   = idx,
            value   = (pred_out * scale_history + center_history)^2
        )
        
        # Combine actual data with predictions
        tbl_1 <- df %>%
            add_column(key = "actual")

        tbl_3 <- pred_tbl %>%
            add_column(key = "predict")

        # Create time_bind_rows() to solve dplyr issue
        time_bind_rows <- function(data_1, data_2, index) {
            index_expr <- enquo(index)
            bind_rows(data_1, data_2) %>%
                as_tbl_time(index = !! index_expr)
        }

        ret <- list(tbl_1, tbl_3) %>%
            reduce(time_bind_rows, index = index) %>%
            arrange(key, index) %>%
            mutate(key = as_factor(key))

        return(ret)
    }
    safe_lstm <- possibly(lstm_prediction, otherwise = NA)
    safe_lstm(data, epochs, ...)
}
```

```{r}
future_pred_1 <- predict_keras_lstm_future(temp1, epochs = 100)
```



```{r}

```
profiling diagnostics filter maybe? 

```{r}
library(skimr)
#look at ts and filter for certain percentage of missing/zero values in order to discard infrequent items? 
```


