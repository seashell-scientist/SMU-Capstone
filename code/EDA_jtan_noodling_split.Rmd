---
title: "Capstone"
author: "Jenna Ford, Christian Nava, Jonathan Tan"
date: "5/21/2020"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
library(rmdformats)
library(tidyverse)  # data manipulaiton
library(data.table)
library(tswge)  # Time series package
library(tseries)  # for Dickey-Fuller test 
library(orcutt)  # for Cochrane-Orcutt test
library(formattable)  # for table formatting
library(GGally)
library(astsa)
library(nnfor)
library(dplyr)
library(ggplot2)
library(changepoint)
library(date)
library(R.devices)
knitr::opts_chunk$set(echo = TRUE,
               prompt = FALSE,
               tidy = TRUE,
               comment = NA,
               message = FALSE,
               warning = FALSE)
```

***
# Time Series Analysis of Corpus Christi Liquor Distribution
***
Jenna Ford, Christian Nava and Jonathan Tan  
May 21, 2020


## Read in Datasets and Combine
***
```{r data} 
#note ask for updated headings csv changes to stay synced
df_17_19 = read.csv("D:/SMU/DS 6120 Capstone A/Bivin CC 2017-19_updated_headings.csv")
df_15_16 = read.csv("D:/SMU/DS 6120 Capstone A/Bivin CC 2015-16_updated_headings.csv")
df_13_14 = read.csv("D:/SMU/DS 6120 Capstone A/Bivin CC 2013-14_updated_headings.csv")
#df_17_19 = read.csv("C:/Users/b007224/Documents/masters_in_data_science/capstone/data/Bivin CC 2017-19_updated_headings.csv")
#df_15_16 = read.csv("C:/Users/b007224/Documents/masters_in_data_science/capstone/data/Bivin CC 2015-16_updated_headings.csv")
#df_13_14 = read.csv("C:/Users/b007224/Documents/masters_in_data_science/capstone/data/Bivin CC 2013-14_updated_headings.csv")

dim(df_17_19)
dim(df_15_16)
dim(df_13_14)

#format date column
df_17_19$date = date.mmddyy(mdy.date(match(substr(df_17_19$Month, 1, 3),month.abb),01,df_17_19$Year),sep="/")
df_15_16$date = date.mmddyy(mdy.date(match(substr(df_15_16$Month, 4, 6),month.abb),01,df_15_16$Year),sep="/")
df_13_14$date = date.mmddyy(mdy.date(match(substr(df_13_14$Month, 1, 3),month.abb),01,df_13_14$Year),sep="/")

df <- rbind(df_17_19,df_15_16,df_13_14)

dim(df)
```
## Check for Missing Data
***
Here we create a function to check for missing data.
```{r missing}
check.for.missing.data <- function(data){
  a = colnames(data)
  b = colSums(is.na(df))  %>% as.data.table
  missing_value_table = cbind(a, b)
  colnames(missing_value_table) = c("Variables","Missing_values")
  missing_value_table = missing_value_table  %>% filter(Missing_values>0)  %>% 
                        mutate("As a % of Total Values" = round(100 * (Missing_values / nrow(df)), 1))  %>% 
                        arrange(desc(Missing_values))
  head(missing_value_table, 20)
}

table_a = check.for.missing.data(data=df)
# display table with first column aligned left all others aligned right
formattable(table_a, align = c("l", rep("r", NCOL(table_a) - 1)))
```

## Keep Only Active Accounts
***

```{r filter}
invisible(df %>% filter(Account_Status != "Closed"))
```

## Drop Variables
***

```{r drop}
drops <- c("Metrics","Year","Month","House","Account_Status","Beverage_Type","Fiscal_Year","Premise","Customer_Street_Address","Customer_City",
           "Customer_Zip_Code","Longitude_Customer","Latitude_Customer","Customer","Vendor","Brand_ID","Brand","Size","Product_ID","Chain","Category",
           "Product_Type_ID","Qty_Per_Case","Alcohol_Proof","X9L_Cases","Dollar_Sales_Per_Case","Dollar_Sales_Per_9L_Case")
df = df[ , !(names(df) %in% drops)]
```

## Variable Formatting
***

```{r formatting}
df$Dollar_Sales = as.numeric(gsub('[$,]', '', df$Dollar_Sales))
str(df)
```

## Products
***

```{r products}
product_type_group = df %>% group_by(Product_Type) %>% tally(sort=TRUE)
dim(product_type_group)
formattable(head(product_type_group), align = c("l", rep("r", NCOL(table_a) - 1)))

product_group = df %>% group_by(Product) %>% tally(sort=TRUE)
dim(product_group)
formattable(head(product_group), align = c("l", rep("r", NCOL(table_a) - 1)))

customer_group = df %>% group_by(Customer_ID) %>% tally(sort=TRUE)
dim(customer_group)
formattable(head(customer_group), align = c("l", rep("r", NCOL(table_a) - 1)))

product_type_product_group = df %>% group_by(Product_Type, Product) %>% tally(sort=TRUE)
dim(product_type_product_group)
formattable(head(product_type_product_group), align = c("l", rep("r", NCOL(table_a) - 1)))

all_group = df %>% group_by(Product_Type, Product, Customer_ID) %>% tally(sort=TRUE)
dim(all_group)
formattable(head(all_group), align = c("l", rep("r", NCOL(table_a) - 1)))
```

## Put Combinations for Forecasting into a Dataframe and Choose 10 Sample Combinations
***

```{r combinations}
# Create file with all possible combinations
combinations0 = as.data.frame(df %>% group_by(Product_Type, Product, Customer_ID) %>% tally(sort=TRUE))
combinations = combinations0 %>% filter(n >= 42)

# Sample combinations
set.seed(123)
sample_combinations = sample_n(combinations,10)
drops <- c("n")
sample_combinations = sample_combinations[ , !(names(sample_combinations) %in% drops)]
formattable(sample_combinations, align = c("l", rep("r", NCOL(table_a) - 1)))
```


```{r}
#temp_store_product_list testing app testing
ctest <- combinations$Customer_ID[1]
x <- combinations[which(combinations$Customer_ID == ctest), ]
ptest <- x$Product[1]

z <- df[which(df$Customer_ID == ctest), ]
y <- z[which(z$Product == ptest), ] #filter down to single product from single store

nacounter <- which(y$STD_Cases == 0) %>% length() %>% sum()
 
ggplot() +
  geom_line(data = y, aes(x = as.Date(date, '%m/%d/%Y'), y = STD_Cases)) 

paste("Number of missing values: " ,nacounter)

summary(y$STD_Cases)
summary(y$Dollar_Sales)
```

```{r}
#ordering dropdown boxes testing
sort(x$Product)
```


tswge doesn't seem to publish through rshiny?? alternatives??
```{r}
#replacement for plotts.sample.wge()
par(mfrow = c(2, 2))
plot(y$STD_Cases, type = 'l', main = '')
acf(y$STD_Cases, main = 'ACF')
spectrum(y$STD_Cases, method = 'pgram', type = 'h')
spectrum(y$STD_Cases, method = 'ar')
```

```{r}
plotts.sample.wge(y$STD_Cases)
```

#gganimate testing
```{r}
#single ts
library(gganimate)
library(gifski)

#add new column for year and month
y$year <- substring(as.Date(y$date, '%m/%d/%Y'), 1, 4)
y$month <- substring(as.Date(y$date, '%m/%d/%Y'), 6, 7)

g1 <- ggplot(data = y, aes(x = Dollar_Sales, y = STD_Cases, color = year, size = Dollar_Sales)) +
  geom_point(alpha = 0.7, stroke = 0)+
  scale_size(range = c(1, 12), guide = 'none') +
  labs(title = 'Standard Cases Sold Per Month', x = 'Month', y = 'Cases') +
  scale_color_brewer(palette = 'Set2') 
g1

anim1 = g1 +
  transition_time(as.Date(date, '%m/%d/%Y'))+
  labs(subtitle = "Date: {frame_time}") +
  shadow_wake(wake_length = 0.1)

animate(anim1, height = 400, width = 700, fps = 30, duration = 10, end_pause = 120, res = 100, renderer = gifski_renderer())
anim_save('test_anim2', path = 'D:/SMU/DS 6120 Capstone A')
#hmm doesn't play everything at once
```

```{r}
#multiple ts testing
#px <- vector()
#for(i in sample_combinations$Customer_ID){
#  print(i)
#  px <- append(px, i)
#  }
px <- sample_combinations$Customer_ID[2]
df2 <- df[which(df$Customer_ID == px), ]

df2$year <- substring(as.Date(df2$date, '%m/%d/%Y'), 1, 4)
df2$month <- substring(as.Date(df2$date, '%m/%d/%Y'), 6, 7)
#as.Date(date, '%m/%d/%Y')
g1 <- ggplot(data = df2, aes(x = as.Date(date, '%m/%d/%Y'), y = STD_Cases, color = Product_Type)) +
  geom_point(alpha = 0.7, stroke = 0)+
  scale_size(range = c(1, 12), guide = 'none') +
  #scale_x_log10() +
  labs(title = 'Standard Cases Sold Per Month', x = 'Dollar Sales', y = 'Cases') +
  scale_color_brewer(palette = 'Set2') + 
  theme(legend.position = 'none') 
  
g1

anim1 = g1 +
  transition_time(as.Date(date, '%m/%d/%Y'))+
  labs(subtitle = "Date: {frame_time}") +
  shadow_wake(wake_length = 0.1)

animate(anim1, height = 400, width = 700, fps = 30, duration = 10, end_pause = 120, res = 100, renderer = gifski_renderer())
anim_save('test_anim2', path = 'D:/SMU/DS 6120 Capstone A')
#hmm doesn't play everything at once
```

# so that doesn't really work, still some issues when displaying  univariate time series

#basic endpoint model fitting, forecast, and display for app
```{r}
aic5.wge(y$STD_Cases, p = 0:5, q = 0:5, type= 'aic')

#the knockoff
model_target = y$STD_Cases
p_range = 0:5
q_range = 0:5

pqc <- expand.grid(p_range, q_range)
aic_holder <- vector()
p_holder <- vector()
q_holder <- vector()

for(i in seq(1, length(pqc$Var1), 1)) {
  #print(pqc[i, ])
  temp_model <- arima(model_target, order = c(pqc[i, 1], 0, pqc[i, 2]), method = "ML")
  #aic_holder <- append(aic_holder, temp_model$aic) #base AIC with k = 2?
  aic_holder <- append(aic_holder, AIC(temp_model, k = log(length(y$STD_Cases))))
  p_holder <- append(p_holder, pqc[i, 1])
  q_holder <- append(q_holder, pqc[i, 2])
}

aic_results <- data.frame(p = p_holder, q = q_holder, aic = aic_holder)
aic_results <- aic_results[order(aic_results$aic), ]
head(aic_results, 5)

#significant difference in aic results from aic5.wge and AIC from base r, calculation differences? 
```

```{r}
n_ahead = 2

print('tswge arima estimation')
a1 <- aic5.wge(y$STD_Cases, p = 0:5, q = 0:5, type= 'aic')
m1 <- est.arma.wge(y$STD_Cases, p = a1[1, 1], q = a1[1, 2])
#m1
fore.aruma.wge(y$STD_Cases, phi = m1$phi, theta = m1$theta, d = 0, s = 0, n.ahead = n_ahead)
```

```{r}
#print('base r arima estimation')
model_target = y
p_range = 0:5
q_range = 0:5

pqc <- expand.grid(p_range, q_range)
aic_holder <- vector()
p_holder <- vector()
q_holder <- vector()

for(i in seq(1, length(pqc$Var1), 1)) {
  #print(pqc[i, ])
  temp_model <- arima(model_target$STD_Cases, order = c(pqc[i, 1], 0, pqc[i, 2]), method = "ML")
  #aic_holder <- append(aic_holder, temp_model$aic) #base AIC with k = 2?
  aic_holder <- append(aic_holder, AIC(temp_model, k = log(length(model_target$STD_Cases))))
  p_holder <- append(p_holder, pqc[i, 1])
  q_holder <- append(q_holder, pqc[i, 2])
}

aic_results <- data.frame(p = p_holder, q = q_holder, aic = aic_holder)
aic_results <- aic_results[order(aic_results$aic), ]

m2 <- arima0(model_target$STD_Cases, order = c(aic_results$p[1], 0, aic_results$q[1]))
m2
pred2 <- predict(m2, n_ahead)
f2 <- append(model_target$STD_Cases, as.vector(pred2$pred))
d_labs <- append(rep('data', length(model_target$STD_Cases)), rep('predictions', length(pred2$pred)))
time_labs <- append(as.Date(y$date, "%m/%d/%Y"), seq.Date(as.Date(model_target$date, "%m/%d/%Y")[length(model_target$date)], by = 'month', length.out = 3)[-1])
time_labs <- sort(time_labs)
ep2 <- data.frame(cases = f2, category = d_labs, date = time_labs)

v2 <- ggplot(data = ep2) + 
  geom_point(aes(x = date, y = cases, color = category))+
  geom_line(aes(x = date, y = cases, color = category)) +
  ggtitle(paste('model of ', y$Product[1]), subtitle = paste('ARIMA (', a1[1, 1], ',', 0, ',', a1[1, 2], ')'))

v2
```

```{r}
ep2
```


```{r}
#mlr testing
fit = lm(STD_Cases~as.Date(date, '%m/%d/%Y'), data = y)
summary(fit)
newdata = data.frame(month = seq.Date(as.Date('1/1/2020', '%m/%d/%Y'), by = 'month', length.out = 6))
predict(fit, newdata = newdata)

aic5.wge(fit$residuals) # AR(1)

est1 = est.arma.wge(fit$residuals, p = 1)

forecasts = fore.arma.wge(fit$residuals,phi = est1$phi,lastn = FALSE,n.ahead = 12)
fors = preds + forecasts$f

plot(fors,type = "l")
```


## Time Series EDA & Forecasting
***

```{r eda1}
# Find data from file for first combination
all_dates = data.frame(date=c("1/1/2013","2/1/2013","3/1/2013","4/1/2013","5/1/2013","6/1/2013","7/1/2013","8/1/2013","9/1/2013","10/1/2013","11/1/2013","12/1/2013",
                              "1/1/2014","2/1/2014","3/1/2014","4/1/2014","5/1/2014","6/1/2014","7/1/2014","8/1/2014","9/1/2014","10/1/2014","11/1/2014","12/1/2014",
                              "1/1/2015","2/1/2015","3/1/2015","4/1/2015","5/1/2015","6/1/2015","7/1/2015","8/1/2015","9/1/2015","10/1/2015","11/1/2015","12/1/2015",
                              "1/1/2016","2/1/2016","3/1/2016","4/1/2016","5/1/2016","6/1/2016","7/1/2016","8/1/2016","9/1/2016","10/1/2016","11/1/2016","12/1/2016",
                              "1/1/2017","2/1/2017","3/1/2017","4/1/2017","5/1/2017","6/1/2017","7/1/2017","8/1/2017","9/1/2017","10/1/2017","11/1/2017","12/1/2017",
                              "1/1/2018","2/1/2018","3/1/2018","4/1/2018","5/1/2018","6/1/2018","7/1/2018","8/1/2018","9/1/2018","10/1/2018","11/1/2018","12/1/2018",
                              "1/1/2019","2/1/2019","3/1/2019","4/1/2019","5/1/2019","6/1/2019","7/1/2019","8/1/2019","9/1/2019","10/1/2019","11/1/2019","12/1/2019"))

date_combinations = merge(all_dates,sample_combinations,all=TRUE)
str(date_combinations)

# Join date/sample combinations with primary dataset to review records
temp = left_join(date_combinations,df)

# Replace missing values for STD_Cases and Dollar_Sales with zeros since no cases were sold those months
temp[is.na(temp)] <- 0
str(temp)

```

```{r sample1, eval=FALSE}
#only one sample combination
sample_combinations1 = sample_combinations[2,]
temp1 = inner_join(temp,sample_combinations1)
product = sample_combinations1$Product
customer = sample_combinations1$Customer_ID
product_type = sample_combinations1$Customer_ID

plot.ts(temp1$STD_Cases, 
        main=c(paste("Standard Case Sales of ", product), 
               paste("for ",customer)),
        xlab="Months",
        ylab="Standard Cases")

par(mfrow = c(2,2))
invisible(acf(temp1$STD_Cases, main="ACF"))
invisible(parzen.wge(temp1$STD_Cases))

invisible(acf(temp1$STD_Cases[0:length(temp1$date)/2], main="ACF for 1st Half of Series"))
invisible(acf(temp1$STD_Cases[(1+length(temp1$date)/2):length(temp1$date)], main="ACF for 2nd Half of Series"))

aic = aic5.wge(temp1$STD_Cases,type="aic")
aic

for (i in 1:5){
  if(aic[i,1] == 0 & aic[i,2] == 0){
    print("One of the top 5 models using BIC was an ARMA(0,0), indicating this series may be white noise.")
  }
  
}
test <- ts(temp1$STD_Cases[1:60])
model1 = invisible(aic.wge(test,type="aic"))
model1_est = invisible(est.arma.wge(test,p=model1$p,q=model1$q))
fore.arma.wge(test,phi = .888408, theta = .7637687, n.ahead = 9,plot=FALSE)
```

```{r loop}
results <- data.frame(Product_Type=integer(),
                      Product=character(),
                      Customer=integer(),
                      ljung_10=double(),
                      ljung_24=double(),
                      ljung_results=character(),
                      top_5_bic=character(),
                      EqualMeans_2_ASE=double(),
                      EqualMeans_3_ASE=double(),
                      EqualMeans_4_ASE=double(),
                      EqualMeans_5_ASE=double(),
                      EqualMeans_6_ASE=double(),
                      EqualMeans_7_ASE=double(),
                      EqualMeans_8_ASE=double(),
                      EqualMeans_9_ASE=double(),
                      EqualMeans_10_ASE=double(),
                      EqualMeans_11_ASE=double(),
                      EqualMeans_12_ASE=double(),
                      ARMA_2_ASE=double(),
                      ARMA_3_ASE=double(),
                      ARMA_4_ASE=double(),
                      ARMA_5_ASE=double(),
                      ARMA_6_ASE=double(),
                      ARMA_7_ASE=double(),
                      ARMA_8_ASE=double(),
                      ARMA_9_ASE=double(),
                      ARMA_10_ASE=double(),
                      ARMA_11_ASE=double(),
                      ARMA_12_ASE=double(),
                      ARIMA_2_ASE=double(),
                      ARIMA_3_ASE=double(),
                      ARIMA_4_ASE=double(),
                      ARIMA_5_ASE=double(),
                      ARIMA_6_ASE=double(),
                      ARIMA_7_ASE=double(),
                      ARIMA_8_ASE=double(),
                      ARIMA_9_ASE=double(),
                      ARIMA_10_ASE=double(),
                      ARIMA_11_ASE=double(),
                      ARIMA_12_ASE=double(),
                      ARIMA_S12_2_ASE=double(),
                      ARIMA_S12_3_ASE=double(),
                      ARIMA_S12_4_ASE=double(),
                      ARIMA_S12_5_ASE=double(),
                      ARIMA_S12_6_ASE=double(),
                      ARIMA_S12_7_ASE=double(),
                      ARIMA_S12_8_ASE=double(),
                      ARIMA_S12_9_ASE=double(),
                      ARIMA_S12_10_ASE=double(),
                      ARIMA_S12_11_ASE=double(),
                      ARIMA_S12_12_ASE=double(),
                      stringsAsFactors = FALSE)

# loop through sample combinations
for(i in 1:10) {
  sample_combinations1 = sample_combinations[i,]
  temp1 = inner_join(temp,sample_combinations1)
  product = sample_combinations1$Product
  customer = sample_combinations1$Customer_ID
  product_type = sample_combinations1$Customer_ID
  
  results[i,"Product_Type"] = product_type
  results[i,"Product"] = as.character(sample_combinations1$Product)
  results[i,"Customer"] = customer
  
  par(mfrow=c(1,1))
  plot.ts(temp1$STD_Cases, 
          main=c(paste("Standard Case Sales of ", product), 
                 paste("for Customer",customer)),
          xlab="Months",
          ylab="Standard Cases")
  
  par(mfrow = c(2,2))
  invisible(acf(temp1$STD_Cases, main="ACF"))
  invisible(parzen.wge(temp1$STD_Cases))

  invisible(acf(temp1$STD_Cases[0:length(temp1$date)/2], main="ACF for 1st Half of Series"))
  invisible(acf(temp1$STD_Cases[(1+length(temp1$date)/2):length(temp1$date)], main="ACF for 2nd Half of Series"))

  sink("file")
  ljung_10 = ljung.wge(temp1$STD_Cases,K=10)
  sink()
  cat("The Ljung-Box test with K=10 has a p-value of",ljung_10$pval,".")
  results[i,"ljung_10"] = ljung_10$pval
  
  sink("file")
  ljung_24 = ljung.wge(temp1$STD_Cases,K=24)
  sink()
  cat("The Ljung-Box test with K=24 has a p-value of",ljung_24$pval,".")
  results[i,"ljung_24"] = ljung_24$pval
  
  if (ljung_10$pval < .05 & ljung_24$pval < .05){
    print("Ljung-Box test results: At a significance level of 0.05, we reject the null hypothesis that this dataset is white noise.")
    results[i,"ljung_results"] = "not white noise"
  } else if (ljung_10$pval > .05 & ljung_24$pval < .05){
    print("Ljung-Box test results: At a significance level of 0.05, the test is inconclusive.")
    results[i,"ljung_results"] = "inconclusive"
  } else if (ljung_10$pval < .05 & ljung_24$pval > .05){
    print("Ljung-Box test results: At a significance level of 0.05, the test is inconclusive.")
    results[i,"ljung_results"] = "inconclusive"
  } else {
    print("Ljung-Box test results: At a significance level of 0.05, we fail to reject the null hypothesis that this dataset is white noise.")
    results[i,"ljung_results"] = "white noise"
  }
  
  sink("file")
  aic = invisible(aic5.wge(temp1$STD_Cases,type="bic"))
  sink()
  
 for (row in 1:nrow(aic)) {
    if(aic[row,1] == 0 & aic[row,2] == 0){
      print("One of the top 5 models using BIC was an ARMA(0,0), indicating this series may be white noise.")
      results[i,"top_5_bic"] = "white noise"
    }
 }

  #Equal Means Model
  for(j in 2:12) {
    trainingSize = 60
    ASEHolder = numeric()
    
    for( k in 1:(84-(trainingSize + j) + 1))
    {
      sink("file")
      model0_mean = mean(temp1$STD_Cases[k:(k+(trainingSize-1))])
      ASE = mean((temp1$STD_Cases[(trainingSize+k):(trainingSize+ k + j - 1)] - model0_mean)^2)
      ASEHolder[k] = ASE
      sink()
    }
    
    WindowedASE = mean(ASEHolder)
    results[i,paste0("EqualMeans_",j,"_ASE")] = WindowedASE
  }

  #ARMA Model
  for(j in 2:12) {
    trainingSize = 60
    ASEHolder = numeric()
    
    for( k in 1:(84-(trainingSize + j) + 1))
    {
      sink("file")
      model1 = invisible(aic.wge(temp1$STD_Cases[k:(k+(trainingSize-1))],type="aic"))
      model1_est = invisible(est.arma.wge(temp1$STD_Cases[k:(k+(trainingSize-1))],p=model1$p,q=model1$q))
      forecasts = fore.aruma.wge(temp1$STD_Cases[k:(k+(trainingSize-1))],phi = model1_est$phi, theta = model1_est$theta, s = 0, d = 0,n.ahead = j,plot=FALSE)
      ASE = mean((temp1$STD_Cases[(trainingSize+k):(trainingSize+ k + j - 1)] - forecasts$f)^2)
      ASEHolder[k] = ASE
      sink()
    }
    
    WindowedASE = mean(ASEHolder)
    results[i,paste0("ARMA_",j,"_ASE")] = WindowedASE

  }
  
  #ARIMA Model with d=1
  nulldev()
  temp2 = artrans.wge(temp1$STD_Cases,1)
  dev.off()
  for(j in 2:12) {
    trainingSize = 60
    ASEHolder = numeric()
    
    for( k in 1:(84-(trainingSize + j) + 1))
    {
      sink("file")
      model1 = invisible(aic.wge(temp2[k:(k+(trainingSize-1-1))],type="aic"))
      model1_est = invisible(est.arma.wge(temp2[k:(k+(trainingSize-1-1))],p=model1$p,q=model1$q))
      forecasts = fore.aruma.wge(temp1$STD_Cases[k:(k+(trainingSize-1))],phi = model1_est$phi, theta = model1_est$theta, s = 0, d = 1,n.ahead = j,plot=FALSE)
      ASE = mean((temp1$STD_Cases[(trainingSize+k):(trainingSize+ k + j - 1)] - forecasts$f)^2)
      ASEHolder[k] = ASE
      sink()
    }
    
    WindowedASE = mean(ASEHolder)
    results[i,paste0("ARIMA_",j,"_ASE")] = WindowedASE

  }
  
  #ARIMA Model with S=12
  nulldev()
  temp2 = artrans.wge(temp1$STD_Cases,phi.tr=c(rep(0,11),1))
  dev.off()
  for(j in 2:12) {
    trainingSize = 60
    ASEHolder = numeric()
    
    for( k in 1:(84-(trainingSize + j) + 1))
    {
      sink("file")
      model1 = invisible(aic.wge(temp2[k:(k+(trainingSize-1-12))],type="aic"))
      model1_est = invisible(est.arma.wge(temp2[k:(k+(trainingSize-1-12))],p=model1$p,q=model1$q))
      forecasts = fore.aruma.wge(temp1$STD_Cases[k:(k+(trainingSize-1))],phi = model1_est$phi, theta = model1_est$theta, s = 12, d = 0,n.ahead = j,plot=FALSE)
      ASE = mean((temp1$STD_Cases[(trainingSize+k):(trainingSize+ k + j - 1)] - forecasts$f)^2)
      ASEHolder[k] = ASE
      sink()
    }
    
    WindowedASE = mean(ASEHolder)
    results[i,paste0("ARIMA_S12_",j,"_ASE")] = WindowedASE

  }
  
}

formattable(results, align = c("l", rep("r", NCOL(table_a) - 1)))
```

arma/arima model writeups

(the arima portion of the pipeline) will have an ARMA, ARIMA with d = 1, and ARIMA with s = 12 fitted to each product/customer record and forecasted 2-12 months out. 


```{r}
# Core Tidyverse
library(tidyverse)
library(glue)
library(forcats)

# Time Series
library(timetk)
library(tidyquant)
library(tibbletime)

# Visualization
library(cowplot)

# Preprocessing
library(recipes)

# Sampling / Accuracy
library(rsample)
library(yardstick) 

# Modeling
library(keras)
library(tfruns)
```

format sample combination ts into tripartate LSTM format
first need ts form, set index and value
cross validation for time series: need to keep the time dependency, can't just shuffle things without losing meaning. create multiple synthetic "futures"? 

```{r}
ex1 <- data.frame(date = strptime((temp1$date), "%d/%m/%Y"), cases = temp1$STD_Cases)
ggplot(ex1, aes(x = date, y = cases)) + geom_line()
```

```{r}
ex1 <- data.frame(date = strptime((temp1$date), "%d/%m/%Y"), cases = temp1$STD_Cases)
ex2 <- tbl_time(ex1, date)

rolling_origin_resamples <- rolling_origin(
  ex2,
  initial    = 24,
  assess     = 12,
  cumulative = FALSE, #means that the resmaples will be the same... size? 
  skip       = 6
)

rolling_origin_resamples

```

backsample vizualization 
```{r}
# Plotting function for a single split
plot_split <- function(split, expand_y_axis = TRUE, 
                       alpha = 1, size = 1, base_size = 14) {
    
    # Manipulate data
    train_tbl <- training(split) %>%
        add_column(key = "training") 
    
    test_tbl  <- testing(split) %>%
        add_column(key = "testing") 
    
    data_manipulated <- bind_rows(train_tbl, test_tbl) %>%
        as_tbl_time(index = date) %>%
        mutate(key = fct_relevel(key, "training", "testing"))
        
    # Collect attributes
    train_time_summary <- train_tbl %>%
        tk_index() %>%
        tk_get_timeseries_summary()
    
    test_time_summary <- test_tbl %>%
        tk_index() %>%
        tk_get_timeseries_summary()
    
    # Visualize
    g <- data_manipulated %>%
        ggplot(aes(x = date, y = cases, color = key)) +
        geom_line(size = size, alpha = alpha) +
        theme_tq(base_size = base_size) +
        scale_color_tq() +
        labs(
          title    = glue("Split: {split$id}"),
          subtitle = glue("{train_time_summary$start} to ", 
                          "{test_time_summary$end}"),
            y = "", x = ""
        ) +
        theme(legend.position = "none") 
    
    if (expand_y_axis) {
        
        time_summary <- ex2 %>% 
            tk_index() %>% 
            tk_get_timeseries_summary()
        
        g <- g +
            scale_x_date(limits = c(time_summary$start, 
                                    time_summary$end))
    }
    
    g
}
```
shows a single split that has 100 years train, 50 test. if you plotted the second split, this entire "window" would move up by 22 years
```{r}
rolling_origin_resamples$splits[[1]] %>%
    plot_split(expand_y_axis = FALSE) +
    theme(legend.position = "bottom")
```

```{r}
#diagnosis cell
split <- rolling_origin_resamples$splits[[1]] 
alpha = 1
size = 1
base_size = 14


# Manipulate data
    train_tbl <- training(split) %>%
        add_column(key = "training") 
    
    test_tbl  <- testing(split) %>%
        add_column(key = "testing") 
    
    data_manipulated <- bind_rows(train_tbl, test_tbl) %>%
        as_tbl_time(index = date) %>%
        mutate(key = fct_relevel(key, "training", "testing"))
    
     # Collect attributes
    train_time_summary <- train_tbl %>%
        tk_index() %>%
        tk_get_timeseries_summary()
    
    test_time_summary <- test_tbl %>%
        tk_index() %>%
        tk_get_timeseries_summary()
    
    # Visualize
    g <- data_manipulated %>%
        ggplot(aes(x = date, y = cases, color = key)) +
        geom_line(size = size, alpha = alpha) +
        theme_tq(base_size = base_size) +
        scale_color_tq() +
        labs(
          title    = glue("Split: {split$id}"),
          subtitle = glue("{train_time_summary$start} to ", 
                          "{test_time_summary$end}"),
            y = "", x = ""
        ) +
        theme(legend.position = "none") 
    g
```

plotting all subdivisions
```{r}
# Plotting function that scales to all splits 
plot_sampling_plan <- function(sampling_tbl, expand_y_axis = FALSE, 
                               ncol = 3, alpha = 1, size = 1, base_size = 14, 
                               title = "Sampling Plan") {
    
    # Map plot_split() to sampling_tbl
    sampling_tbl_with_plots <- sampling_tbl %>%
        mutate(gg_plots = map(splits, plot_split, 
                              expand_y_axis = expand_y_axis,
                              alpha = alpha, base_size = base_size))
    
    # Make plots with cowplot
    plot_list <- sampling_tbl_with_plots$gg_plots 
    
    p_temp <- plot_list[[1]] + theme(legend.position = "bottom")
    legend <- get_legend(p_temp)
    
    p_body  <- plot_grid(plotlist = plot_list, ncol = ncol)
    
    p_title <- ggdraw() + 
        draw_label(title, size = 14, fontface = "bold", 
                   colour = palette_light()[[1]])
    
    g <- plot_grid(p_title, p_body, legend, ncol = 1, 
                   rel_heights = c(0.05, 1, 0.05))
    
    g
    
}

rolling_origin_resamples %>%
    plot_sampling_plan(expand_y_axis = F, ncol = 3, alpha = 1, size = 1, base_size = 10, 
                       title = "Backtesting Strategy: Rolling Origin Sampling Plan")
```
so we can see the 'slices' of the data are pretty unhelpful when looking at a mostly sparse time series

looking at an individual slice to test lstm
```{r}
example_split    <- rolling_origin_resamples$splits[[6]]
example_split_id <- rolling_origin_resamples$id[[6]]

plot_split(example_split, expand_y_axis = FALSE, size = 0.5) + 
  theme(legend.position = 'bottom') + ggtitle(glue("Split: {example_split_id}"))
```

data setup
```{r}
split_length <- length(analysis(example_split)$date) #number of total datapoints in the split
split_point <- round(split_length*.6) #take the first 60% of the sample as train

df_trn <- analysis(example_split)[1:split_point, , drop = FALSE] 
df_val <- analysis(example_split)[(split_point + 1):split_length, , drop = FALSE] #take the rest data as validation/test
df_tst <- assessment(example_split) #but this test data is... ??? 

df <- bind_rows(
  df_trn %>% add_column(key = "training"),
  df_val %>% add_column(key = "validation"),
  df_tst %>% add_column(key = "testing")
) %>%
  as_tbl_time(index = date)

df
```

LSTM works best with centered and scaled input data, use recipies package step_center, step_scale, and step_sqrt to center data on ???, scale it, and remove outliers
```{r}
rec_obj <- recipe(cases ~ ., df) %>%
    step_sqrt(cases) %>%
    step_center(cases) %>%
    step_scale(cases) %>%
    prep()

df_processed_tbl <- bake(rec_obj, df)

df_processed_tbl

```
record center "adjustments" so you can reverse the values for interpretation later
```{r}
center_history <- rec_obj$steps[[2]]$means["cases"]
scale_history <- rec_obj$steps[[3]]$sds["cases"]

c("center", center_history, "scale", scale_history)
```

Keras LSTM expects 3d array with num_samples, num_timesteps, and num_features
num_samples is the number of observations in set, goes in the arg "batch_size"
num_timesteps is the length of hidden state??? 
 - this hidden state is dependent on how far ahead you want to forecast, usually just literally input the number of steps to "step ahead" 
num-features is number of predictors. for this univariate example, = 1

```{r}
n_timesteps <- 3
n_predictions <- n_timesteps
batch_size <- 6

# functions used
build_matrix <- function(tseries, overall_timesteps) {
  t(sapply(1:(length(tseries) - overall_timesteps + 1), function(x) 
    tseries[x:(x + overall_timesteps - 1)]))
}

reshape_X_3d <- function(X) {
  dim(X) <- c(dim(X)[1], dim(X)[2], 1)
  X
}

# extract values from data frame
train_vals <- df_processed_tbl %>%
  filter(key == "training") %>%
  select(cases) %>%
  pull()
valid_vals <- df_processed_tbl %>%
  filter(key == "validation") %>%
  select(cases) %>%
  pull()
test_vals <- df_processed_tbl %>%
  filter(key == "testing") %>%
  select(cases) %>%
  pull()


# build the windowed matrices
train_matrix <-
  build_matrix(train_vals, n_timesteps + n_predictions)
valid_matrix <-
  build_matrix(valid_vals, n_timesteps + n_predictions)
test_matrix <- build_matrix(test_vals, n_timesteps + n_predictions)

# separate matrices into training and testing parts
# also, discard last batch if there are fewer than batch_size samples
# (a purely technical requirement)
X_train <- train_matrix[, 1:n_timesteps]
y_train <- train_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_train <- X_train[1:(nrow(X_train) %/% batch_size * batch_size), ]
y_train <- y_train[1:(nrow(y_train) %/% batch_size * batch_size), ]

X_valid <- valid_matrix[, 1:n_timesteps]
y_valid <- valid_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_valid <- X_valid[1:(nrow(X_valid) %/% batch_size * batch_size), ]
y_valid <- y_valid[1:(nrow(y_valid) %/% batch_size * batch_size), ]

X_test <- test_matrix[, 1:n_timesteps]
y_test <- test_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_test <- X_test[1:(nrow(X_test) %/% batch_size * batch_size), ]
y_test <- y_test[1:(nrow(y_test) %/% batch_size * batch_size), ]
# add on the required third axis
X_train <- reshape_X_3d(matrix(X_train)) #note, x_train is coming out as a list rather than a matrix??? should be a 3d matrix
X_valid <- reshape_X_3d(matrix(X_valid)) #does matrixing the list work?? 
X_test <- reshape_X_3d(matrix(X_test))

y_train <- reshape_X_3d(matrix(y_train))
y_valid <- reshape_X_3d(matrix(y_valid))
y_test <- reshape_X_3d(matrix(y_test))
```

building the actual LSTM model
```{r}
FLAGS <- flags(
  flag_boolean('stateful', FALSE), #??? 
  flag_boolean('stack_layers', FALSE), #adding layers does ??? guides says it doesn't help for this task
  flag_integer('batch_size', 10), #samples fed to model per iteration
  flag_integer('n_timesteps', 12), #size of predictions, which is the same as size of hidden state
  flag_integer('n_epochs', 100), #how many epochs??? (time intervals?)
  flag_numeric('dropout', 0.2), #drops .2 units for linear transformation of inputs
  flag_numeric('recurrent_dropout', 0.2), #drops .2 units for linear transformation??? 
  flag_string('loss', 'logcosh'), #loss function used, logcosh worked better than MSE
  flag_string('optimizer_type', 'sgd'), #use stochastic gradient descent for optimization (other options include adam or rmsprop)
  flag_integer('n_units', 128), #size of lstm layer
  flag_numeric('lr', 0.003), #learning rate?
  flag_numeric('momentum', 0.9), #parameter for SGD optimizer
  flag_integer('patience', 10) #early stopping callback check
)

# the number of predictions we'll make equals the length of the hidden state
n_predictions <- FLAGS$n_timesteps
# how many features = predictors we have
n_features <- 1

optimizer <- switch(FLAGS$optimizer_type,
                    sgd = optimizer_sgd(lr = FLAGS$lr, 
                    momentum = FLAGS$momentum))

#early stop function if the loss starts "stalling"? 
callbacks <- list(
  callback_early_stopping(patience = FLAGS$patience)
)

```
note, requires CUDA installation to be in/on the system PATH

actually create model
```{r}
model <- keras_model_sequential()

model %>%
  layer_lstm(
    units = FLAGS$n_units, 
    # the first layer in a model needs to know the shape of the input data
    batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),
    dropout = FLAGS$dropout,
    recurrent_dropout = FLAGS$recurrent_dropout,
    # by default, an LSTM just returns the final state
    return_sequences = TRUE
  ) %>% time_distributed(layer_dense(units = 1))

model %>%
  compile(
    loss = FLAGS$loss,
    optimizer = optimizer,
    # in addition to the loss, Keras will inform us about current 
    # MSE while training
    metrics = list("mean_squared_error")
  )

history <- model %>% fit(
  x          = X_train,
  y          = y_train,
  validation_data = list(X_valid, y_valid),
  batch_size = FLAGS$batch_size,
  epochs     = FLAGS$n_epochs,
  callbacks = callbacks
)
```
convergence(?) varies strongly based on batch size, steps, skips, ect

```{r}
history$metrics
```


look at model vs training set
```{r}
pred_train <- model %>%
  predict(X_train, batch_size = FLAGS$batch_size) %>%
  .[, , 1]

# Retransform values to original scale
pred_train <- (pred_train * scale_history + center_history) ^2
compare_train <- df %>% filter(key == "training")
```

how to output what the actual forecasts are??? RMSE/metric evaluation?? 

```{r}
# build a dataframe that has both actual and predicted values
for (i in 1:nrow(pred_train)) {
  varname <- paste0("pred_train", i)
  compare_train <-
    mutate(compare_train,!!varname := c(
      rep(NA, FLAGS$n_timesteps + i - 1),
      pred_train[i,],
      rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)
    ))
}
```

```{r}
#future forecasts using temp1$STD_Cases


predict_keras_lstm_future <- function(data, epochs = 300, ...) {
    
    lstm_prediction <- function(data, epochs, ...) {
        
        # 5.1.2 Data Setup (MODIFIED)
        df <- temp1
        
        # 5.1.3 Preprocessing
        rec_obj <- recipe(STD_Cases ~ ., df) %>%
            step_sqrt(STD_Cases) %>%
            step_center(STD_Cases) %>%
            step_scale(STD_Cases) %>%
            prep()
        
        df_processed_tbl <- bake(rec_obj, df)
        
        center_history <- rec_obj$steps[[2]]$means["STD_Cases"]
        scale_history  <- rec_obj$steps[[3]]$sds["STD_Cases"]
        
        # 5.1.4 LSTM Plan
        lag_setting  <- 12 # = nrow(df_tst)
        batch_size   <- 36
        train_length <- 84
        tsteps       <- 1
        epochs       <- epochs
        
        # 5.1.5 Train Setup (MODIFIED)
        lag_train_tbl <- df_processed_tbl %>%
            mutate(STD_Cases_lag = lag(STD_Cases, n = lag_setting)) %>%
            filter(!is.na(STD_Cases_lag)) %>%
            tail(train_length)
        
        x_train_vec <- lag_train_tbl$STD_Cases_lag
        x_train_arr <- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))
        
        y_train_vec <- lag_train_tbl$STD_Cases
        y_train_arr <- array(data = y_train_vec, dim = c(length(y_train_vec), 1))
        
        x_test_vec <- y_train_vec %>% tail(lag_setting)
        x_test_arr <- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))
                
        # 5.1.6 LSTM Model
        model <- keras_model_sequential()

        model %>%
            layer_lstm(units            = 50, 
                       input_shape      = c(tsteps, 1), 
                       batch_size       = batch_size,
                       return_sequences = TRUE, 
                       stateful         = TRUE) %>% 
            layer_lstm(units            = 50, 
                       return_sequences = FALSE, 
                       stateful         = TRUE) %>% 
            layer_dense(units = 1)
        
        model %>% 
            compile(loss = 'mae', optimizer = 'adam')
        
        # 5.1.7 Fitting LSTM
        for (i in 1:epochs) {
            model %>% fit(x          = x_train_arr, 
                          y          = y_train_arr, 
                          batch_size = batch_size,
                          epochs     = 1, 
                          verbose    = 1, 
                          shuffle    = FALSE)
            
            model %>% reset_states()
            cat("Epoch: ", i)
            
        }
        
        # 5.1.8 Predict and Return Tidy Data (MODIFIED)
        # Make Predictions
        pred_out <- model %>% 
            predict(x_test_arr, batch_size = batch_size) %>%
            .[,1] #local py_call_error? arg mismatch here
        
        # Make future index using tk_make_future_timeseries()
        idx <- data %>%
            tk_index() %>%
            tk_make_future_timeseries(n_future = lag_setting)
        
        # Retransform values
        pred_tbl <- tibble(
            index   = idx,
            value   = (pred_out * scale_history + center_history)^2
        )
        
        # Combine actual data with predictions
        tbl_1 <- df %>%
            add_column(key = "actual")

        tbl_3 <- pred_tbl %>%
            add_column(key = "predict")

        # Create time_bind_rows() to solve dplyr issue
        time_bind_rows <- function(data_1, data_2, index) {
            index_expr <- enquo(index)
            bind_rows(data_1, data_2) %>%
                as_tbl_time(index = !! index_expr)
        }

        ret <- list(tbl_1, tbl_3) %>%
            reduce(time_bind_rows, index = index) %>%
            arrange(key, index) %>%
            mutate(key = as_factor(key))

        return(ret)
    }
    safe_lstm <- possibly(lstm_prediction, otherwise = NA)
    safe_lstm(data, epochs, ...)
}
```

```{r}
future_pred_1 <- predict_keras_lstm_future(temp1, epochs = 100)
```



```{r}

```
profiling diagnostics filter maybe? 

```{r}
library(skimr)
#look at ts and filter for certain percentage of missing/zero values in order to discard infrequent items? 
```

don't know where this went, fished it out of github history
```{r}
#random forest bag of word approach testing
library(ranger)
library(randomForest)
library(caret)

n_ahead <- 3

y$date <- as.Date(y$date, '%m/%d/%Y')
target_df = y

tlen = length(target_df$STD_Cases)

#test against last year/n periods data
sample_train <- target_df[1:(tlen-n_ahead), ]
sample_test <- target_df[(tlen-n_ahead +1):tlen, ]
rf1 <- randomForest(STD_Cases ~., data = sample_train, na.action = na.exclude)

pred = predict(rf1, newdata = sample_test)

rf_ase <- mean((sample_test$STD_Cases - pred)^2)

ggplot() + 
  geom_line(aes(x = sort(y$date), y = y$STD_Cases), col = 'black') +
  geom_line(aes(x = sort(y$date), y = c(rep(NA, (tlen-n_ahead)), array(pred))), col = 'red') +
  ggtitle(paste('Random Forest', n_ahead, 'Month ASE: ', rf_ase))
  
```

```{r}
#rolling ase RF?
n_ahead <- 24

y$date <- as.Date(y$date, '%m/%d/%Y')
target_df = y
sample_train <- target_df[1:(tlen-n_ahead), ]
sample_test <- target_df[(tlen-n_ahead +1):tlen, ]
rf1 <- randomForest(STD_Cases ~., data = sample_train, na.action = na.exclude)
pred = predict(rf1, newdata = sample_test)
rf_ase <- mean((sample_test$STD_Cases - pred)^2)
ggplot() + 
  geom_line(aes(x = sort(y$date), y = y$STD_Cases), col = 'black') +
  geom_line(aes(x = sort(y$date), y = c(rep(NA, (tlen-n_ahead)), array(pred))), col = 'red') +
  ggtitle(paste('Random Forest', n_ahead, 'Month ASE: ', rf_ase))

```

NN section

```{r}
#ugghh try to functionalize it
#functionalize the model fit, accept ts/vector object, length ahead, reps, hidden layers in vector form,
#internal: split TS into test/train by the specified length ahead, fit model to train, predict n_ahead, compare to test to get ASE
#output: print(mlp object), plot(mlp object), ggplot the TS, predictions in different color, output the predictions as a separate vector? Optional plot showing with display = TRUE 

#assuming that y is a dataframe of the narrowed query data
target_df <- y

#nn complete
nnc <- function(tvector, n_ahead, reps, hd_vector, display){
  #date_line <- as.Date(target_df$date, '%m/%d/%Y') #add date back in after it works
  tlen <- length(tvector)
  sample_train <- target_df[1:(tlen-n_ahead), ]
  sample_test <- target_df[(tlen-n_ahead +1):tlen, ]
  
  ts_train<- as.ts(sample_train$STD_Cases)
  fit_mlp <- mlp(ts_train, reps = reps, hd = hd_vector)
  
  mlp_pred <- forecast(fit_mlp, h = n_ahead)
  mlp_ase <- mean((sample_test$STD_Cases - mlp_pred$mean)^2)
  
  r1 <- ggplot() + 
    geom_line(aes(x = sort(target_df$date), y = target_df$STD_Cases), col = 'black') +
    geom_line(aes(x = sort(target_df$date), y = c(rep(NA, (tlen-n_ahead)), mlp_pred$mean)), col = 'red') +
    ggtitle(paste('MLP', n_ahead, 'Month ASE: ', mlp_ase), subtitle =paste(target_df$Product[1], 'Purchases By Cust ID: ', target_df$Customer_ID[1]))
  
  if(display == TRUE){
    print(fit_mlp)
    plot(fit_mlp)
    show(r1)}
  
  return(list(mlp_pred$mean, mlp_ase))
}
```

```{r}
#test function
x <- nnc(y$STD_Cases, 3, 25, c(5, 10, 15, 5), TRUE)
x
#returns object as list with predictions and ase
```


```{r}
#rolling ase jenna style?? might be computationally expensive to recalc the model for each window? time later
#just need to switch around the input data and store each ase
#as far as i understand it, we are starting at 12 months, then forecasting from 12 months ago 2, 3, 4, 5, 6, ect months out, then compositing those ase as rolling ase? but retraining on every single new dataset??

repetitions = 25
layer_structure = c(24, 12, 6)

#structure to start12month rolling ase, can i just pass along 12/11 different n_aheads? 
windows <- seq(2, 12, 1) #list of n_aheads
nn_ase_holder <- vector()
nn_pred_holder <- vector()

for(i in windows){
  temp_results <- nnc(y$STD_Cases, i, repetitions, layer_structure, display = FALSE)
  nn_ase_holder <- append(nn_ase_holder, temp_results[[2]]) #double square brackets because list of lists
  nn_pred_holder <- append(nn_pred_holder, temp_results[[1]])
}
paste('mean 12 month Rolling ASE (KINDA) = ', mean(nn_ase_holder))
```

non-updating rolling ase
```{r}
#take the same fitted model and gauge its effectiveness across multiple time spans of the same time series

reps = 10
hd_vector = c(5, 5, 5) #completely arbitrary as of now, look up common practices for this later
n_ahead = 6

target_df <- y
target_vector <- target_df$STD_Cases
tlen <- length(target_vector)
win_train_size = 8
win_test_size = n_ahead
step_size = 6
show_fit = TRUE #t/f for fit output

#initialize start points for each window
start_points <- seq(1, tlen, step_size)
step_sequence_truncated = start_points[1:(length(start_points)-5)]
win_ase_holder = vector()
win_pred_holder <- matrix(nrow = length(start_points), ncol = n_ahead) #empty matrix to hold predictions for plotting later
m_indexer = 1
for(i in step_sequence_truncated){ #point generation is a bit wonky, cut off the last iteration to avoid feeding in NAS
  #print(c(i, i+win_train_size, i+win_train_size+1, i+win_train_size+win_test_size))
  win_train <- target_df[i:(i+win_train_size), ]
  win_test <- target_df[(i+win_train_size+1):(i+win_train_size+win_test_size), ]
  
  win_mlp <- mlp(as.ts(win_train$STD_Cases), reps = reps, hd = hd_vector, outplot = show_fit) 
  
  
  #why doesn't it generalize on win_train????? 
  
  
  win_pred<- forecast(win_mlp, h = n_ahead)
  win_ase <- mean((win_test$STD_Cases-win_pred$mean)^2)
  
  win_pred_holder[m_indexer, ] <- win_pred$mean
  m_indexer = m_indexer + 1 #iterate along matrix rows to fill with predictions
  win_ase_holder <- append(win_ase_holder, win_ase)
}

#trim by invalid ase measures to remove out of bounds predictions
valid_bounds <- (dim(win_pred_holder)[1]-sum(is.na(win_ase_holder))) #should return the row limit of data that was still within the bounds of the data without nas
win_pred_holder <- win_pred_holder[1:valid_bounds, ]
win_ase_holder <- win_ase_holder[1:valid_bounds]

paste('NN mean rolling ASE :', mean(win_ase_holder))

#cool graph time
#arrange in dataframe with dates, data, and labels?? 
viz_frame = data.frame(y$date, y$STD_Cases, rep('original', length(y$date)))
names(viz_frame) <- c('date', 'cases', 'data_type')
#append additional generated frames here with datatype labels for easy ggplotting?

#generate datasets with filler nas, bind to dataframe with full dates and labels, rbind to vizframe?
wincount = 1
#for(i in start_points[1:valid_bounds]){
for(i in step_sequence_truncated){
  #print(c(i, i+win_train_size, i+win_train_size+1, i+win_train_size+win_test_size))
  space_before <- i+win_train_size
  space_after <- length(target_df$STD_Cases) - (i+win_train_size+win_test_size)
  spaced_preds <- c(rep(NA, space_before), win_pred_holder[wincount, ], rep(NA, space_after))
  
  #print(spaced_preds)
  #create dataframe with each spaced preds that corresponds to date and label it in the third column
  df_temp <- data.frame('date' = target_df$date, 'cases' = spaced_preds, 'data_type' = rep(paste('window ', wincount, ' predictions'), length(target_df$date)))
  wincount = wincount + 1
  viz_frame <- rbind(viz_frame, df_temp)
}
#there we go graph by color is easier now
nn_graph <- ggplot(data = viz_frame) +
  geom_line(aes(x = date, y = cases, color = data_type)) +
  ggtitle('MLP Rolling ASE', subtitle = paste('NN mean rolling ASE :', mean(win_ase_holder)))+
  xlab('Date') + ylab('Cases purchased')
show(nn_graph)
```

```{r}
viz_frame
```


